{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca4f9de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "def generate_regular_ldpc(n, dv, dc):\n",
    "    \"\"\"\n",
    "    Generates a Regular LDPC Matrix using MacKay & Neal's construction (Algorithm 2).\n",
    "\n",
    "    Parameters:\n",
    "    n (int): Code length (number of columns / variable nodes).\n",
    "    dv (int): Variable node degree (number of 1s per column).\n",
    "    dc (int): Check node degree (number of 1s per row).\n",
    "\n",
    "    Returns:\n",
    "    tuple: (var_nodes, check_nodes) adjacency lists representing the Tanner Graph.\n",
    "           Returns (None, None) if generation fails.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 1: Calculate constraints ---\n",
    "    # According to the conservation of edges: n * dv = m * dc\n",
    "    # We must calculate the number of rows (m) derived from these parameters.\n",
    "    if (n * dv) % dc != 0:\n",
    "        print(\"Error: Invalid parameters. (n * dv) must be divisible by dc.\")\n",
    "        return None, None\n",
    "\n",
    "    m = int((n * dv) / dc)\n",
    "\n",
    "    print(f\"Generating ({dv}, {dc})-Regular LDPC Code.\")\n",
    "    print(f\"Structure: {n} Columns, {m} Rows.\")\n",
    "\n",
    "    # Safety mechanism: The algorithm is probabilistic and might hit a \"dead end\".\n",
    "    # We allow a maximum number of retries before giving up.\n",
    "    max_retries = 100\n",
    "    attempt = 0\n",
    "\n",
    "    # --- Step 2: The Construction Loop ---\n",
    "    while attempt < max_retries:\n",
    "        attempt += 1\n",
    "\n",
    "        # Initialize the Adjacency Lists (The Tanner Graph Structure)\n",
    "        # var_nodes[j] stores the list of row indices connected to column j.\n",
    "        var_nodes = [[] for _ in range(n)]\n",
    "\n",
    "        # check_nodes[i] stores the list of column indices connected to row i.\n",
    "        check_nodes = [[] for _ in range(m)]\n",
    "\n",
    "        # Keep track of how many connections each row currently has.\n",
    "        # Initially, all rows have 0 connections.\n",
    "        current_row_degrees = [0] * m\n",
    "\n",
    "        success = True # Flag to verify if this attempt completes successfully\n",
    "\n",
    "        # --- Step 3: Column-by-Column Filling ---\n",
    "        for col_idx in range(n):\n",
    "\n",
    "            # Find \"Candidate Rows\":\n",
    "            # Identify all rows that have not yet reached the limit 'dc'.\n",
    "            candidate_rows = []\n",
    "            for row_idx in range(m):\n",
    "                if current_row_degrees[row_idx] < dc:\n",
    "                    candidate_rows.append(row_idx)\n",
    "\n",
    "            # --- Deadlock Check ---\n",
    "            # We must place 'dv' ones in the current column.\n",
    "            # If valid candidate rows are fewer than 'dv', we cannot satisfy the constraint.\n",
    "            if len(candidate_rows) < dv:\n",
    "                success = False\n",
    "                break # Break the inner loop to restart the process\n",
    "\n",
    "            # --- Random Selection ---\n",
    "            # Randomly select 'dv' distinct rows from the valid candidates.\n",
    "            chosen_rows = random.sample(candidate_rows, dv)\n",
    "\n",
    "            # --- Update Graph Connections ---\n",
    "            for row_idx in chosen_rows:\n",
    "                # 1. Update Variable Node (Column -> Rows)\n",
    "                # This corresponds to \"Bit-to-Check\" connections.\n",
    "                var_nodes[col_idx].append(row_idx)\n",
    "\n",
    "                # 2. Update Check Node (Row -> Columns)\n",
    "                # This corresponds to \"Check-to-Bit\" connections.\n",
    "                check_nodes[row_idx].append(col_idx)\n",
    "\n",
    "                # 3. Increment the weight counter for this row\n",
    "                current_row_degrees[row_idx] += 1\n",
    "\n",
    "        # --- Step 4: Final Validation ---\n",
    "        if success:\n",
    "            print(f\"Success! Matrix generated on attempt {attempt}.\")\n",
    "            return var_nodes, check_nodes\n",
    "\n",
    "    # If we exit the while loop, it means we failed after max_retries.\n",
    "    print(\"Failed to generate matrix. Try increasing max_retries or checking parameters.\")\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f73cdac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test with the parameters from the slides or a small example\n",
    "N = 12   # Number of columns\n",
    "Dv = 3   # Ones per column\n",
    "Dc = 4   # Ones per row\n",
    "\n",
    "# Execute the function\n",
    "v_nodes, c_nodes = generate_regular_ldpc(N, Dv, Dc)\n",
    "\n",
    "# Visualization\n",
    "\n",
    "def visualize_matrix(n, m, var_nodes):\n",
    "    \"\"\"\n",
    "    可视化 LDPC 矩阵 H 的稀疏结构\n",
    "    \"\"\"\n",
    "    # 1. 先把邻接表转换回普通的 0/1 矩阵 (仅用于绘图)\n",
    "    H = np.zeros((m, n))\n",
    "    for col_idx, connected_rows in enumerate(var_nodes):\n",
    "        for row_idx in connected_rows:\n",
    "            H[row_idx, col_idx] = 1\n",
    "\n",
    "    # 2. 使用 spy 函数绘制\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.spy(H, markersize=5, color='black') # markersize控制点的大小\n",
    "    plt.title(f\"Sparsity Pattern of H Matrix ({n}x{m})\")\n",
    "    plt.xlabel(\"Variable Nodes (Columns)\")\n",
    "    plt.ylabel(\"Check Nodes (Rows)\")\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "# --- 测试 ---\n",
    "# 假设你已经有了 v_nodes 和 c_nodes\n",
    "visualize_matrix(N, len(c_nodes), v_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff4563f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def spa_decoder(var_nodes, check_nodes, llr_channel, max_iter=50):\n",
    "    \"\"\"\n",
    "    Log-Domain Sum-Product Algorithm (SPA) Decoder\n",
    "\n",
    "    Args:\n",
    "        var_nodes: List of lists (from Part 1)\n",
    "        check_nodes: List of lists (from Part 1)\n",
    "        llr_channel: Received LLRs from channel (numpy array of shape (N,))\n",
    "        max_iter: Maximum number of iterations\n",
    "\n",
    "    Returns:\n",
    "        estimated_bits: Decoded bits (0 or 1)\n",
    "        success: Boolean, True if syndrome check passed\n",
    "    \"\"\"\n",
    "    n = len(var_nodes)\n",
    "    m = len(check_nodes)\n",
    "\n",
    "    # --- 1. Initialize Message Structure ---\n",
    "    # We use a dictionary or sparse matrix to store messages.\n",
    "    # key: (row_idx, col_idx), value: message\n",
    "    # L_vc: Variable-to-Check messages (Phi)\n",
    "    # L_cv: Check-to-Variable messages (Psi)\n",
    "    L_vc = {}\n",
    "    L_cv = {}\n",
    "\n",
    "    # Initial state: Messages from Variable to Check = Channel LLR\n",
    "    for col in range(n):\n",
    "        for row in var_nodes[col]:\n",
    "            L_vc[(row, col)] = llr_channel[col]\n",
    "\n",
    "    # --- 2. Iteration Loop ---\n",
    "    for iteration in range(max_iter):\n",
    "\n",
    "        # === Step A: Check Node Update (Psi) ===\n",
    "        # Corresponds to Slide 30\n",
    "        for row in range(m):\n",
    "            # Get all variable nodes connected to this check node\n",
    "            connected_cols = check_nodes[row]\n",
    "\n",
    "            # Calculate tanh product\n",
    "            # Optimization: Calculate the product of all tanh values first,\n",
    "            # then divide by the current tanh to get the \"exclude current\" product.\n",
    "            tanh_vals = [np.tanh(L_vc[(row, col)] / 2.0) for col in connected_cols]\n",
    "            \n",
    "            for i, col in enumerate(connected_cols):\n",
    "                # Exclude self (Leave-one-out)\n",
    "                # Note: If tanh_vals[i] is 0, division would error.\n",
    "                # A robust approach is to re-multiply excluding self.\n",
    "                prod_exclude_me = 1.0\n",
    "                for j, other_col in enumerate(connected_cols):\n",
    "                    if i != j:\n",
    "                        prod_exclude_me *= tanh_vals[j]\n",
    "\n",
    "                # Prevent numerical overflow/underflow (clipping to open interval (-1, 1))\n",
    "                prod_exclude_me = np.clip(prod_exclude_me, -0.999999, 0.999999)\n",
    "\n",
    "                # Calculate L_cv (Psi)\n",
    "                L_cv[(row, col)] = 2.0 * np.arctanh(prod_exclude_me)\n",
    "\n",
    "        # === Step B: Decision & Syndrome Check ===\n",
    "        # Corresponds to Slide 31-32\n",
    "        # Calculate Posterior LLR\n",
    "        L_posterior = np.zeros(n)\n",
    "        estimated_bits = np.zeros(n, dtype=int)\n",
    "\n",
    "        for col in range(n):\n",
    "            # Initial channel information\n",
    "            sum_val = llr_channel[col]\n",
    "            # Add information from all connected check nodes\n",
    "            for row in var_nodes[col]:\n",
    "                sum_val += L_cv[(row, col)]\n",
    "\n",
    "            L_posterior[col] = sum_val\n",
    "\n",
    "            # Hard Decision\n",
    "            # LLR < 0 implies higher probability of 1 (For BPSK +1/-1 mapping, usually LLR < 0 corresponds to bit 1)\n",
    "            # Note: This depends on your LLR definition. Slide 29 states \"if Gamma >= 0 then x=0 else x=1\"\n",
    "            if L_posterior[col] < 0:\n",
    "                estimated_bits[col] = 1\n",
    "            else:\n",
    "                estimated_bits[col] = 0\n",
    "\n",
    "        # Syndrome Check: z = x * H^T\n",
    "        # Check if every parity check equation is satisfied (sum(bits) % 2 == 0)\n",
    "        syndrome_valid = True\n",
    "        for row in range(m):\n",
    "            parity = 0\n",
    "            for col in check_nodes[row]:\n",
    "                parity += estimated_bits[col]\n",
    "            if parity % 2 != 0:\n",
    "                syndrome_valid = False\n",
    "                break\n",
    "\n",
    "        if syndrome_valid:\n",
    "            return estimated_bits, True\n",
    "\n",
    "        # === Step C: Variable Node Update (Phi) ===\n",
    "        # Corresponds to Slide 32\n",
    "        for col in range(n):\n",
    "            for row in var_nodes[col]:\n",
    "                # Simple approach: Total Sum - Message from this row (Check node)\n",
    "                # L_vc = L_total - L_cv_from_this_row\n",
    "                # Note: L_total here uses the L_posterior calculated in this round.\n",
    "                # L_posterior contains Channel + All Checks.\n",
    "                # Therefore, subtracting the current Check gives the \"Extrinsic\" information.\n",
    "                L_vc[(row, col)] = L_posterior[col] - L_cv[(row, col)]\n",
    "\n",
    "    # If max_iter reached without success\n",
    "    return estimated_bits, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623795e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def test_spa_decoder():\n",
    "    print(\"=== Starting Unit Test (Slide 33-35) ===\")\n",
    "\n",
    "    # 1. Manually construct the H matrix from Slide 33 (4 rows, 6 columns)\n",
    "    # Be very careful with indices here; they must strictly correspond to the positions of 1s in the matrix\n",
    "\n",
    "    # var_nodes (Column -> Row)\n",
    "    # Col 0: Connected to Row 0, 2\n",
    "    # Col 1: Connected to Row 0, 1\n",
    "    # Col 2: Connected to Row 1, 3\n",
    "    # Col 3: Connected to Row 0, 3\n",
    "    # Col 4: Connected to Row 1, 2\n",
    "    # Col 5: Connected to Row 2, 3\n",
    "    var_nodes = [\n",
    "        [0, 2], [0, 1], [1, 3], [0, 3], [1, 2], [2, 3]\n",
    "    ]\n",
    "\n",
    "    # check_nodes (Row -> Column)\n",
    "    # Row 0: Connected to Col 0, 1, 3\n",
    "    # Row 1: Connected to Col 1, 2, 4\n",
    "    # Row 2: Connected to Col 0, 4, 5\n",
    "    # Row 3: Connected to Col 2, 3, 5\n",
    "    check_nodes = [\n",
    "        [0, 1, 3], [1, 2, 4], [0, 4, 5], [2, 3, 5]\n",
    "    ]\n",
    "\n",
    "    # 2. Input Channel LLR (Slide 33)\n",
    "    gamma = np.array([-0.5, 2.5, -4.0, 5.0, -3.5, 2.5])\n",
    "\n",
    "    print(f\"Input LLR: {gamma}\")\n",
    "\n",
    "    # 3. Run the decoder\n",
    "    # Note: To see the intermediate process, you might need to temporarily modify spa_decoder\n",
    "    # Make it print(L_posterior) at the end of each loop iteration\n",
    "    # Or run for 1 iteration, then 2, then 3 to check results\n",
    "\n",
    "    # --- Test Iteration 1 ---\n",
    "    print(\"\\n--- Testing Iteration 1 ---\")\n",
    "    # This step requires your decoder to return posterior LLRs (L_posterior) for comparison\n",
    "    # If your decoder only returns bits, it is recommended to add a debug mode to return LLRs\n",
    "    # Here we assume manual verification of the printed values\n",
    "    decoded_bits, success = spa_decoder(var_nodes, check_nodes, gamma, max_iter=1)\n",
    "\n",
    "    expected_iter1 = np.array([-0.2676, 5.0334, -3.7676, 2.2783, -6.2217, -0.7173])\n",
    "    print(f\"Expected LLRs (Slide 34): {expected_iter1}\")\n",
    "    print(\"Please check if your decoder output is close to the above values\")\n",
    "\n",
    "    # --- Test Iteration 3 (Final Result) ---\n",
    "    print(\"\\n--- Testing Full Decoding (3 Iterations) ---\")\n",
    "    decoded_bits, success = spa_decoder(var_nodes, check_nodes, gamma, max_iter=3)\n",
    "\n",
    "    expected_bits = np.array([0, 0, 1, 0, 1, 1]) # Corresponding to LLR > 0 is 0, < 0 is 1\n",
    "    # Note: The transmitted codeword x in Slide 33 is [0, 0, 1, 0, 1, 1]\n",
    "\n",
    "    print(f\"Your decoding result:     {decoded_bits}\")\n",
    "    print(f\"Expected decoding result: {expected_bits}\")\n",
    "\n",
    "    if np.array_equal(decoded_bits, expected_bits):\n",
    "        print(\"✅ Final bit check passed!\")\n",
    "    else:\n",
    "        print(\"❌ Final bit check failed!\")\n",
    "\n",
    "    if success:\n",
    "        print(\"✅ Syndrome Check Passed\")\n",
    "    else:\n",
    "        print(\"❌ Syndrome Check Failed\")\n",
    "\n",
    "# Run test\n",
    "test_spa_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e761c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "=== Starting Unit Test (Slide 33-35) ===\n",
    "Input LLR: [-0.5  2.5 -4.   5.  -3.5  2.5]\n",
    "\n",
    "--- Testing Iteration 1 ---\n",
    "Expected LLRs (Slide 34): [-0.2676  5.0334 -3.7676  2.2783 -6.2217 -0.7173]\n",
    "Please check if your decoder output is close to the above values\n",
    "\n",
    "--- Testing Full Decoding (3 Iterations) ---\n",
    "Your decoding result:     [0 0 1 0 1 1]\n",
    "Expected decoding result: [0 0 1 0 1 1]\n",
    "✅ Final bit check passed!\n",
    "✅ Syndrome Check Passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f3136a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import csv  # New: Used for saving files\n",
    "import os   # New: Used to check if file exists\n",
    "\n",
    "def simulation_part3_with_save():\n",
    "    # 1. Set parameters\n",
    "    N = 1000\n",
    "    dv = 4\n",
    "    dc = 8\n",
    "    R = 1 - (dv/dc) \n",
    "    \n",
    "    # Filename\n",
    "    filename = f\"ldpc_results_N{N}_dv{dv}_dc{dc}.csv\"\n",
    "\n",
    "    # Generate matrix (Part 1)\n",
    "    print(f\"Generating Code (N={N}, dv={dv}, dc={dc})...\")\n",
    "    var_nodes, check_nodes = generate_regular_ldpc(N, dv, dc)\n",
    "    if var_nodes is None: \n",
    "        print(\"Matrix generation failed!\")\n",
    "        return\n",
    "\n",
    "    # 2. Set SNR range (dB)\n",
    "    snr_range_db = np.arange(0,3.5,0.5) # 0, 0.5, ..., 3.0\n",
    "    \n",
    "    ber_results = [] \n",
    "    \n",
    "    print(f\"\\nResults will be saved to: {filename}\")\n",
    "    print(\"\\nStarting Monte Carlo Simulation...\")\n",
    "    print(f\"{'SNR(dB)':<10} | {'Frames':<10} | {'Bit Errs':<10} | {'BER':<15}\")\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "    # If file does not exist, write the header first\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    with open(filename, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow(['SNR_dB', 'Frames', 'Bit_Errors', 'BER', 'FER'])\n",
    "\n",
    "    # 3. Outer loop: Iterate over SNR\n",
    "    for snr_db in snr_range_db:\n",
    "        \n",
    "        snr_linear = 10**(snr_db / 10.0)\n",
    "        sigma = np.sqrt(1 / (2 * R * snr_linear))\n",
    "        \n",
    "        frame_count = 0\n",
    "        bit_error_count = 0\n",
    "        frame_error_count = 0\n",
    "        \n",
    "        # Stop conditions\n",
    "        max_frames = 10000\n",
    "        min_frame_errors = 50\n",
    "        \n",
    "        # 4. Inner loop\n",
    "        while frame_error_count < min_frame_errors and frame_count < max_frames:\n",
    "            frame_count += 1\n",
    "            \n",
    "            # --- A. Modulation & All-zero assumption ---\n",
    "            tx_signal = np.ones(N) \n",
    "            \n",
    "            # --- B. Add noise ---\n",
    "            noise = np.random.randn(N) * sigma\n",
    "            rx_signal = tx_signal + noise\n",
    "            \n",
    "            # --- C. Calculate LLR ---\n",
    "            llr_channel = (2 * rx_signal) / (sigma ** 2)\n",
    "            \n",
    "            # --- D. Decoding ---\n",
    "            decoded_bits, success = spa_decoder(var_nodes, check_nodes, llr_channel, max_iter=50)\n",
    "            \n",
    "            # --- E. Count errors ---\n",
    "            errs = np.sum(decoded_bits)\n",
    "            bit_error_count += errs\n",
    "            if errs > 0:\n",
    "                frame_error_count += 1\n",
    "                \n",
    "        # Calculate BER\n",
    "        ber = bit_error_count / (frame_count * N)\n",
    "        fer = frame_error_count / frame_count\n",
    "        ber_results.append(ber)\n",
    "        \n",
    "        print(f\"{snr_db:<10.2f} | {frame_count:<10} | {bit_error_count:<10} | {ber:<15.2e}\")\n",
    "\n",
    "        # --- Key: Write to file in real-time ---\n",
    "        with open(filename, mode='a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([snr_db, frame_count, bit_error_count, ber, fer])\n",
    "\n",
    "    # 5. Plotting\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.semilogy(snr_range_db, ber_results, 'b-o', label=f'LDPC ({N},{dv},{dc})')\n",
    "    plt.grid(True, which=\"both\")\n",
    "    plt.xlabel('Eb/N0 (dB)')\n",
    "    plt.ylabel('Bit Error Rate (BER)')\n",
    "    plt.title('LDPC Performance over BIAWGNC')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Run simulation with save functionality\n",
    "simulation_part3_with_save()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
